{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"diabetes_dataset.csv\")\n",
    "\n",
    "X = data[\"BMI\"].values    \n",
    "y = data[\"Age\"].values    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# z-score normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize feature for faster convergence\n",
    "X = (X - X.mean()) / X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m =   # initialise slope (weight)\n",
    "c =   # initialise intercept (bias)\n",
    "\n",
    "L = 0.01    # learning rate\n",
    "epochs = 1000   # number of iterations\n",
    "n = len(X)\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = # calculate the predicted value of y\n",
    "    error = # calculate the error \n",
    "    \n",
    "    # Compute gradients\n",
    "    dm = (2/n) * sum(X.flatten() * error)\n",
    "    dc = (2/n) * sum(error)\n",
    "    \n",
    "    # Update parameters\n",
    "    m -= L * dm\n",
    "    c -= L * dc\n",
    "    \n",
    "# Print every 100 steps\n",
    "    if i % 100 == 0:\n",
    "        mse = np.mean(error**2)\n",
    "        print(f\"Epoch {i}: m={m:.4f}, c={c:.4f}, MSE={mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Model: y =\", round(m,3), \"* x +\", round(c,3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
